#!/bin/bash
# AgenticAI V4 - Script de t√©l√©chargement des mod√®les Ollama
# Usage: ./scripts/download-models.sh

set -e

echo "üöÄ AgenticAI V4 - T√©l√©chargement des mod√®les Ollama"
echo "=================================================="

# V√©rifier si Ollama est install√©
if ! command -v ollama &> /dev/null; then
    echo "‚ùå Ollama n'est pas install√©. Veuillez l'installer depuis https://ollama.ai"
    exit 1
fi

# V√©rifier si Ollama est en cours d'ex√©cution
if ! curl -s http://localhost:11434/api/version &> /dev/null; then
    echo "‚ö†Ô∏è  Ollama ne semble pas √™tre en cours d'ex√©cution"
    echo "   D√©marrage d'Ollama..."
    ollama serve &
    sleep 5
fi

echo ""
echo "üì• T√©l√©chargement des mod√®les LLM..."
echo "-----------------------------------"

# LLM Principal
echo "1/7 - qwen2.5:14b (LLM principal, multilingue)"
ollama pull qwen2.5:14b

# LLM Rapide
echo "2/7 - phi4:latest (LLM rapide pour t√¢ches simples)"
ollama pull phi4:latest

# LLM Code
echo "3/7 - qwen2.5-coder:7b (G√©n√©ration de code)"
ollama pull qwen2.5-coder:7b

# Embeddings
echo ""
echo "üì• T√©l√©chargement du mod√®le d'embeddings..."
echo "-----------------------------------"
echo "4/7 - nomic-embed-text:latest (Embeddings long context)"
ollama pull nomic-embed-text:latest

# Multimodal
echo ""
echo "üì• T√©l√©chargement du mod√®le multimodal..."
echo "-----------------------------------"
echo "5/7 - llava:13b (Vision + Langage)"
ollama pull llava:13b

# ASR
echo ""
echo "üì• T√©l√©chargement du mod√®le ASR..."
echo "-----------------------------------"
echo "6/7 - whisper:large-v3 (Transcription audio)"
if ollama list | grep -q "whisper:large-v3"; then
    echo "‚úÖ whisper:large-v3 d√©j√† install√©"
else
    echo "‚ö†Ô∏è  whisper:large-v3 n'est pas disponible via Ollama"
    echo "   Utilisez whisper localement ou via API externe"
fi

# Mod√®les alternatifs
echo ""
echo "üì• Mod√®les alternatifs (optionnels)..."
echo "-----------------------------------"
echo "7/7 - llama3.3:70b (Si vous avez beaucoup de RAM/VRAM)"
read -p "T√©l√©charger llama3.3:70b? (tr√®s gros mod√®le) [y/N] " -n 1 -r
echo
if [[ $REPLY =~ ^[Yy]$ ]]; then
    ollama pull llama3.3:70b
else
    echo "‚è≠Ô∏è  Ignor√©"
fi

echo ""
echo "‚úÖ T√©l√©chargement termin√©!"
echo ""
echo "üìã Mod√®les install√©s:"
ollama list

echo ""
echo "üéâ Tous les mod√®les requis sont pr√™ts!"
echo "   Vous pouvez maintenant d√©marrer AgenticAI V4"
echo ""
echo "Prochaines √©tapes:"
echo "  1. V√©rifier la configuration: cat .env"
echo "  2. D√©marrer les services: docker compose up -d"
echo "  3. D√©marrer le backend: ./scripts/run-backend.sh"
